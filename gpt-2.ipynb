{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5601259,"sourceType":"datasetVersion","datasetId":3089480},{"sourceId":12718703,"sourceType":"datasetVersion","datasetId":8038844}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom typing import Optional, Tuple\nimport math \nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:38.575366Z","iopub.execute_input":"2025-08-09T17:26:38.575616Z","iopub.status.idle":"2025-08-09T17:26:40.119814Z","shell.execute_reply.started":"2025-08-09T17:26:38.575595Z","shell.execute_reply":"2025-08-09T17:26:40.119244Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Set up GPT config","metadata":{}},{"cell_type":"code","source":"class GPTConfig:\n    def __init__(\n        self,\n        vocab_size: int = 13025,        \n        d_model: int = 768,\n        n_layers: int = 12,\n        n_heads: int = 12,\n        d_ff: Optional[int] = None,\n        max_seq_len: int = 64,           \n        dropout: float = 0.1,\n        tie_word_embeddings: bool = True,\n        use_bias_in_proj: bool = True,\n    ):\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.d_ff = d_ff or 4 * d_model\n        self.max_seq_len = max_seq_len\n        self.dropout = dropout\n        self.tie_word_embeddings = tie_word_embeddings\n        self.use_bias_in_proj = use_bias_in_proj","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.121046Z","iopub.execute_input":"2025-08-09T17:26:40.121398Z","iopub.status.idle":"2025-08-09T17:26:40.126144Z","shell.execute_reply.started":"2025-08-09T17:26:40.121377Z","shell.execute_reply":"2025-08-09T17:26:40.125557Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Building Transformer from scratch","metadata":{}},{"cell_type":"code","source":"def causal_mask(sz: int, device: torch.device) -> torch.Tensor:\n    # Returns (1, 1, sz, sz) mask with -inf where future positions are\n    mask = torch.triu(torch.ones((sz, sz), device=device), diagonal=1).bool()\n    return mask  # shape (sz, sz) boolean mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.126778Z","iopub.execute_input":"2025-08-09T17:26:40.127041Z","iopub.status.idle":"2025-08-09T17:26:40.141549Z","shell.execute_reply.started":"2025-08-09T17:26:40.127018Z","shell.execute_reply":"2025-08-09T17:26:40.140860Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class MultiheadAttn(nn.Module):\n  def __init__(self,cfg):\n        super(MultiheadAttn,self).__init__()\n        assert cfg.d_model % cfg.n_heads == 0, \"d_model must be divisible by n_heads\"\n        self.n_heads = cfg.n_heads\n        self.head_dim = cfg.d_model // cfg.n_heads\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n        \n\n        self.qkv_proj = nn.Linear(in_features=cfg.d_model,out_features=3 * cfg.d_model,bias=cfg.use_bias_in_proj)\n        self.out_proj = nn.Linear(in_features=cfg.d_model,out_features=cfg.d_model)\n        self.attn_dropout = nn.Dropout(cfg.dropout)\n        self.proj_dropout = nn.Dropout(cfg.dropout)\n  def forward(self,x : torch.Tensor, attn_mask:Optional[torch.Tensor] =None) -> torch.Tensor:\n      batch_size, max_seq, d_model = x.size()\n      qkv = self.qkv_proj(x)\n      qkv = qkv.reshape(batch_size,max_seq,3,self.n_heads,self.head_dim)\n      q,k,v = qkv.unbind(dim =2)\n      \n      q = q.permute(0,2,1,3)\n      k = k.permute(0,2,1,3) \n      v = v.permute(0,2,1,3)\n      # scaled dot-product \n      attn_scores = torch.matmul(q,k.transpose(-2,-1)) * self.scale\n      if attn_mask is None:\n          mask = causal_mask(max_seq,device=x.device)\n          attn_scores = attn_scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n      else :\n          attn_scores = attn_scores.masked_fill(attn_mask, float(\"-inf\"))\n      attn_probs = F.softmax(attn_scores, dim=-1)\n      attn_probs = self.attn_dropout(attn_probs)\n      \n      out = torch.matmul(attn_probs,v)\n      out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, max_seq, -1)\n      out = self.out_proj(out)\n      out = self.proj_dropout(out)\n      return out ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.142979Z","iopub.execute_input":"2025-08-09T17:26:40.143192Z","iopub.status.idle":"2025-08-09T17:26:40.160934Z","shell.execute_reply.started":"2025-08-09T17:26:40.143175Z","shell.execute_reply":"2025-08-09T17:26:40.160243Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"cfg = GPTConfig()\nmodel = MultiheadAttn(cfg)\nx = torch.randn(2, 5, cfg.d_model)\n\nprint(\"Input shape:\", x.shape)\nout = model(x)\nprint(\"Output shape:\", out.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.161535Z","iopub.execute_input":"2025-08-09T17:26:40.161718Z","iopub.status.idle":"2025-08-09T17:26:40.206649Z","shell.execute_reply.started":"2025-08-09T17:26:40.161703Z","shell.execute_reply":"2025-08-09T17:26:40.206103Z"}},"outputs":[{"name":"stdout","text":"Input shape: torch.Size([2, 5, 768])\nOutput shape: torch.Size([2, 5, 768])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Building Multi-Layer Perceptron**","metadata":{}},{"cell_type":"code","source":"class Multi_Layer_Perceptron(nn.Module):\n  def __init__(self,cfg):\n    super(Multi_Layer_Perceptron,self).__init__()\n    self.d_model = cfg.d_model\n    self.d_ff = cfg.d_ff\n    self.net = nn.Sequential(\n        nn.Linear(self.d_model,self.d_ff),\n        nn.GELU(),\n        nn.Linear(self.d_ff,self.d_model)\n    )\n  def forward(self,x:torch.Tensor) -> torch.Tensor:\n    return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.207282Z","iopub.execute_input":"2025-08-09T17:26:40.207510Z","iopub.status.idle":"2025-08-09T17:26:40.211842Z","shell.execute_reply.started":"2025-08-09T17:26:40.207484Z","shell.execute_reply":"2025-08-09T17:26:40.211132Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"cfg = GPTConfig()\nmodel = Multi_Layer_Perceptron(cfg)\nx = torch.randn(2, 5, cfg.d_model)\n\nprint(\"Input shape:\", x.shape)\nout = model(x)\nprint(\"Output shape:\", out.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.212525Z","iopub.execute_input":"2025-08-09T17:26:40.212758Z","iopub.status.idle":"2025-08-09T17:26:40.279058Z","shell.execute_reply.started":"2025-08-09T17:26:40.212736Z","shell.execute_reply":"2025-08-09T17:26:40.278427Z"}},"outputs":[{"name":"stdout","text":"Input shape: torch.Size([2, 5, 768])\nOutput shape: torch.Size([2, 5, 768])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# **Building Layer Normalization**","metadata":{}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n  def __init__(self,cfg,eps = 1e-5):\n    super().__init__()\n    self.eps = eps\n    self.gamma = nn.Parameter(torch.ones(cfg.d_model))\n    self.beta = nn.Parameter(torch.zeros(cfg.d_model))\n  def forward(self,x:torch.Tensor):\n    mean = x.mean(dim=-1,keepdim=True)\n    var = x.var(dim=-1,keepdim=True,unbiased=False)\n    x_norm = (x-mean) / torch.sqrt(var + self.eps)\n    return self.gamma * x_norm + self.beta\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.279700Z","iopub.execute_input":"2025-08-09T17:26:40.279924Z","iopub.status.idle":"2025-08-09T17:26:40.285234Z","shell.execute_reply.started":"2025-08-09T17:26:40.279905Z","shell.execute_reply":"2025-08-09T17:26:40.284478Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"cfg = GPTConfig()\nmodel = LayerNormalization(cfg)\nx = torch.randn(2, 5, cfg.d_model)\n\nprint(\"Input shape:\", x.shape)\nout = model(x)\nprint(\"Output shape:\", out.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.286005Z","iopub.execute_input":"2025-08-09T17:26:40.286684Z","iopub.status.idle":"2025-08-09T17:26:40.304241Z","shell.execute_reply.started":"2025-08-09T17:26:40.286622Z","shell.execute_reply":"2025-08-09T17:26:40.303662Z"}},"outputs":[{"name":"stdout","text":"Input shape: torch.Size([2, 5, 768])\nOutput shape: torch.Size([2, 5, 768])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# **Building Transformer Decoder Block**","metadata":{}},{"cell_type":"code","source":"class TransformerDecoderBlock(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.norm1 = LayerNormalization(cfg)\n        self.MultiheadAttn = MultiheadAttn(cfg)\n        self.norm2 = LayerNormalization(cfg)\n        self.ff = Multi_Layer_Perceptron(cfg)\n        self.dropout1 = nn.Dropout(cfg.dropout)\n        self.dropout2 = nn.Dropout(cfg.dropout)\n\n    def forward(self, x, attn_mask=None): \n        norm1 = self.norm1(x)\n        attn = self.MultiheadAttn(norm1, attn_mask=attn_mask)\n        attn = self.dropout1(attn)\n        x = self.norm2(attn + x)\n        ff = self.ff(x)\n        ff = self.dropout2(ff)\n        return ff + x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.306493Z","iopub.execute_input":"2025-08-09T17:26:40.307287Z","iopub.status.idle":"2025-08-09T17:26:40.317188Z","shell.execute_reply.started":"2025-08-09T17:26:40.307266Z","shell.execute_reply":"2025-08-09T17:26:40.316574Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"cfg = GPTConfig()\nmodel = TransformerDecoderBlock(cfg)\nx = torch.randn(2, 5, cfg.d_model)\n\nprint(\"Input shape:\", x.shape)\nout = model(x)\nprint(\"Output shape:\", out.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.317771Z","iopub.execute_input":"2025-08-09T17:26:40.317947Z","iopub.status.idle":"2025-08-09T17:26:40.401871Z","shell.execute_reply.started":"2025-08-09T17:26:40.317934Z","shell.execute_reply":"2025-08-09T17:26:40.401012Z"}},"outputs":[{"name":"stdout","text":"Input shape: torch.Size([2, 5, 768])\nOutput shape: torch.Size([2, 5, 768])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# **Building GPT Model**","metadata":{}},{"cell_type":"markdown","source":"- Iteratively predict and sample the next token: Runs the model on the current sequence, takes the last-step logits, applies temperature scaling and optional top-k filtering, then samples one token from the probability distribution.\n- Append the new token and repeat until done: Adds the token to the sequence, truncates if too long, and stops early if the EOS token is generated or the maximum new token count is reached.\n","metadata":{}},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()  # Fixed: Use modern super() syntax\n        self.cfg = cfg  # Store config for later use\n\n        # Token and positional embeddings\n        self.token_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb = nn.Parameter(torch.zeros(1, cfg.max_seq_len, cfg.d_model))\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([TransformerDecoderBlock(cfg) for _ in range(cfg.n_layers)])\n\n        # Final layer norm and head\n        self.ln_f = LayerNormalization(cfg)  # Fixed: Renamed from norm1 to ln_f for clarity\n        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n        # Weight tying (optional)\n        if cfg.tie_word_embeddings:\n            self.head.weight = self.token_emb.weight\n\n        self._init_weights()\n\n    def _init_weights(self):\n        # Initialize weights\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n        # Special initialization for positional embeddings\n        nn.init.normal_(self.pos_emb, mean=0.0, std=0.02)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        batch_size, seq_len = input_ids.size()\n\n        # Embeddings\n        tok_emb = self.token_emb(input_ids)  # (B, T, d_model)\n        pos_emb = self.pos_emb[:, :seq_len, :]  # (1, T, d_model)  # Fixed: Use seq_len instead of undefined T\n        x = tok_emb + pos_emb  # (B, T, d_model)\n\n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x, attn_mask=attention_mask)\n\n        # Final layer norm and projection\n        x = self.ln_f(x)\n        logits = self.head(x)  # (B, T, vocab_size)\n        return logits\n\n    @torch.no_grad()\n    def generate(\n        self,\n        input_ids: torch.LongTensor,\n        max_new_tokens: int = 50,\n        temperature: float = 1.0,\n        top_k: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n    ) -> torch.LongTensor:\n        \"\"\"\n        Autoregressive text generation with optional:\n        - Temperature scaling\n        - Top-k filtering\n        - Early stopping via EOS token\n        \"\"\"\n        self.eval()\n        generated = input_ids.clone()\n\n        for _ in range(max_new_tokens):\n            # Truncate if sequence grows too long\n            if generated.size(1) > self.cfg.max_seq_len:\n                generated = generated[:, -self.cfg.max_seq_len:]\n\n            # Forward pass\n            logits = self(generated)  # (B, T, V)\n            next_logits = logits[:, -1, :] / max(temperature, 1e-8)  # (B, V)\n\n            # Top-k filtering\n            if top_k is not None:\n                topk_vals, topk_idx = torch.topk(next_logits, min(top_k, next_logits.size(-1)), dim=-1)\n                next_logits[next_logits < topk_vals[:, -1].unsqueeze(-1)] = float('-inf')\n\n            # Sample next token\n            probs = F.softmax(next_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            generated = torch.cat([generated, next_token], dim=1)\n\n            # Early stopping\n            if eos_token_id is not None and (next_token == eos_token_id).all():\n                break\n\n        return generated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.402808Z","iopub.execute_input":"2025-08-09T17:26:40.403086Z","iopub.status.idle":"2025-08-09T17:26:40.414071Z","shell.execute_reply.started":"2025-08-09T17:26:40.403066Z","shell.execute_reply":"2025-08-09T17:26:40.413344Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"cfg = GPTConfig()\nmodel = GPTModel(cfg)\n\ninput_ids = torch.randint(0, cfg.vocab_size, (2, 5))\nprint(\"Input IDs:\", input_ids)\n\nlogits = model(input_ids)\nprint(\"Logits shape:\", logits.shape)\n\ngenerated = model.generate(input_ids, max_new_tokens=5, top_k=10)\nprint(\"Generated IDs:\", generated)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:40.415030Z","iopub.execute_input":"2025-08-09T17:26:40.415643Z","iopub.status.idle":"2025-08-09T17:26:42.319126Z","shell.execute_reply.started":"2025-08-09T17:26:40.415614Z","shell.execute_reply":"2025-08-09T17:26:42.318230Z"}},"outputs":[{"name":"stdout","text":"Input IDs: tensor([[12458,  3286,  1744,  8664,  3337],\n        [ 3875,  7280,  7907,  9068,  1165]])\nLogits shape: torch.Size([2, 5, 13025])\nGenerated IDs: tensor([[12458,  3286,  1744,  8664,  3337, 10896,  8384,  1035,  8384,  1035],\n        [ 3875,  7280,  7907,  9068,  1165,  7481, 12288,  7717,  1204,  6904]])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Data Preprocesing","metadata":{}},{"cell_type":"code","source":"# import kagglehub\n\n# # Download latest version\n# path = kagglehub.dataset_download(\"noahpersaud/89k-chatgpt-conversations\")\n\n# print(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:42.320027Z","iopub.execute_input":"2025-08-09T17:26:42.320564Z","iopub.status.idle":"2025-08-09T17:26:42.324029Z","shell.execute_reply.started":"2025-08-09T17:26:42.320533Z","shell.execute_reply":"2025-08-09T17:26:42.323247Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Structuring Dataset into X and y, when X is \"Hello, Good morning\", y is Good Morning Chatgpt\", and so on. Thus, making a model to predict and generate text.","metadata":{}},{"cell_type":"code","source":"# import json\n# import pandas as pd\n# import os\n\n# INPUT_FILE = \"chatlogs.jsonl\"  \n# SEQUENCE_LENGTH = 5               \n# OUTPUT_FILE = \"word_level_dataset.csv\"\n# MAX_PAIRS = 100000  # limit to 100k pairs\n\n# def load_json_file(filename):\n#     with open(filename, \"r\", encoding=\"utf-8\") as f:\n#         try:\n#             data = json.load(f)  # try load as single JSON object/array\n#             if isinstance(data, dict):\n#                 data = [data]\n#         except json.JSONDecodeError:\n#             # fallback: try JSON Lines format\n#             f.seek(0)\n#             data = [json.loads(line) for line in f if line.strip()]\n#     return data\n\n# def extract_all_texts(data):\n#     texts = []\n#     for item in data:\n#         for turn in item.get(\"conversation\", []):\n#             msg = turn.get(\"message\", \"\").strip()\n#             if msg:\n#                 texts.append(msg)\n#     return texts\n\n# def create_word_pairs(words, seq_length):\n#     X, Y = [], []\n#     for i in range(len(words) - seq_length):\n#         x_seq = words[i : i + seq_length - 1]  # first n-1 words\n#         y_seq = words[i + 1 : i + seq_length]  # shifted sequence\n#         X.append(\" \".join(x_seq))\n#         Y.append(\" \".join(y_seq))\n#     return X, Y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:42.324792Z","iopub.execute_input":"2025-08-09T17:26:42.325053Z","iopub.status.idle":"2025-08-09T17:26:42.340460Z","shell.execute_reply.started":"2025-08-09T17:26:42.325027Z","shell.execute_reply":"2025-08-09T17:26:42.339805Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# def main():\n#     if not os.path.exists(INPUT_FILE):\n#         print(f\"Error: {INPUT_FILE} not found.\")\n#         return\n\n#     print(f\"Loading dataset from {INPUT_FILE}...\")\n#     data = load_json_file(INPUT_FILE)\n\n#     print(\"Extracting messages...\")\n#     all_texts = extract_all_texts(data)\n#     print(f\"Total messages: {len(all_texts)}\")\n\n#     print(\"Tokenizing...\")\n#     words = \" \".join(all_texts).split()\n\n#     print(f\"Creating (X, Y) pairs with sequence length {SEQUENCE_LENGTH}...\")\n#     X, Y = create_word_pairs(words, SEQUENCE_LENGTH)\n\n#     # Limit to MAX_PAIRS for easier training\n#     if len(X) > MAX_PAIRS:\n#         X = X[:MAX_PAIRS]\n#         Y = Y[:MAX_PAIRS]\n\n#     print(f\"Total training pairs after truncation: {len(X)}\")\n#     df = pd.DataFrame({\"input\": X, \"target\": Y})\n#     df.to_csv(OUTPUT_FILE, index=False)\n\n#     print(f\"Saved dataset to {OUTPUT_FILE}\")\n\n# if __name__ == \"__main__\":\n#     main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:42.341193Z","iopub.execute_input":"2025-08-09T17:26:42.341470Z","iopub.status.idle":"2025-08-09T17:26:42.356718Z","shell.execute_reply.started":"2025-08-09T17:26:42.341446Z","shell.execute_reply":"2025-08-09T17:26:42.355995Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Making vocabulary encoder and decoder  & Creating Dataset","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\n# ==== CONFIG ====\nINPUT_CSV = \"/kaggle/input/processing-dataset/word_level_dataset.csv\"      # Your raw CSV input file\nVOCAB_PATH = \"/kaggle/working/vocab.json\"                  # Where to save vocab\nINV_VOCAB_PATH = \"/kaggle/working/inv_vocab.json\"          # Where to save inverse vocab\nENCODED_CSV = \"/kaggle/working/encoded_dataset.csv\"        # Where to save encoded dataset\nTRAIN_CSV = \"/kaggle/working/train_encoded.csv\"            # Train split CSV\nVAL_CSV = \"/kaggle/working/val_encoded.csv\"                # Validation split CSV\n\nSPECIAL_TOKENS = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\nMAX_SEQ_LEN = 5                            # Max sequence length for dataset padding\nBATCH_SIZE = 200                            # Batch size for DataLoader\nVAL_RATIO = 0.1                           # 10% validation split\n# =================\n\ndef build_vocab(csv_path):\n    df = pd.read_csv(csv_path)\n    all_words = []\n    for col in [\"input\", \"target\"]:\n        for seq in df[col]:\n            all_words.extend(seq.split())\n\n    vocab = {token: idx for idx, token in enumerate(SPECIAL_TOKENS)}\n    for word in set(all_words):\n        if word not in vocab:\n            vocab[word] = len(vocab)\n    return vocab\n\ndef save_vocab(vocab, vocab_path, inv_vocab_path):\n    with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(vocab, f, ensure_ascii=False, indent=2)\n    inv_vocab = {str(idx): word for word, idx in vocab.items()}\n    with open(inv_vocab_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(inv_vocab, f, ensure_ascii=False, indent=2)\n\ndef encode_sequence(seq, vocab):\n    return [vocab[\"<BOS>\"]] + [vocab.get(word, vocab[\"<UNK>\"]) for word in seq.split()] + [vocab[\"<EOS>\"]]\n\ndef encode_dataset(input_csv, output_csv, vocab):\n    df = pd.read_csv(input_csv)\n    df[\"input_ids\"] = df[\"input\"].apply(lambda x: encode_sequence(x, vocab))\n    df[\"target_ids\"] = df[\"target\"].apply(lambda x: encode_sequence(x, vocab))\n    df.to_csv(output_csv, index=False)\n\ndef split_dataset(encoded_csv_path, train_csv_path, val_csv_path, val_ratio=0.1, random_state=42):\n    df = pd.read_csv(encoded_csv_path)\n    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)  # shuffle\n\n    train_df, val_df = train_test_split(df, test_size=val_ratio, random_state=random_state)\n    train_df.to_csv(train_csv_path, index=False)\n    val_df.to_csv(val_csv_path, index=False)\n\n    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n\nclass GPTDataset(Dataset):\n    def __init__(self, csv_file, vocab, max_seq_len):\n        self.vocab = vocab\n        self.max_seq_len = max_seq_len\n        self.df = pd.read_csv(csv_file)\n        self.inputs = self.df[\"input_ids\"].apply(self._parse_ids).tolist()\n        self.targets = self.df[\"target_ids\"].apply(self._parse_ids).tolist()\n        self.inputs = [self._pad_or_truncate(seq) for seq in self.inputs]\n        self.targets = [self._pad_or_truncate(seq) for seq in self.targets]\n\n    def _parse_ids(self, s):\n        if isinstance(s, str) and s.startswith(\"[\"):\n            return json.loads(s.replace(\"'\", '\"'))\n        return []\n\n    def _pad_or_truncate(self, seq):\n        if len(seq) > self.max_seq_len:\n            return seq[:self.max_seq_len]\n        else:\n            pad_len = self.max_seq_len - len(seq)\n            return seq + [self.vocab[\"<PAD>\"]] * pad_len\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        input_ids = torch.tensor(self.inputs[idx], dtype=torch.long)\n        target_ids = torch.tensor(self.targets[idx], dtype=torch.long)\n        return input_ids, target_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:42.357472Z","iopub.execute_input":"2025-08-09T17:26:42.357696Z","iopub.status.idle":"2025-08-09T17:26:43.120104Z","shell.execute_reply.started":"2025-08-09T17:26:42.357672Z","shell.execute_reply":"2025-08-09T17:26:43.119319Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Training Dataset","metadata":{}},{"cell_type":"markdown","source":"Using data pararel -> to speed up our training process , becuase we have to t4 *2 gpu","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.optim import AdamW\n\ndef decode_batch(batch_ids, inv_vocab):\n    decoded = []\n    for seq in batch_ids.cpu().numpy():\n        words = []\n        for idx in seq:\n            word = inv_vocab.get(str(idx), \"<UNK>\")\n            if word == \"<EOS>\":\n                break\n            if word not in [\"<PAD>\", \"<BOS>\"]:\n                words.append(word)\n        # Add this to handle empty outputs\n        if not words:\n            words = [\"[EMPTY]\"]\n        decoded.append(\" \".join(words))\n    return decoded\n\ndef train_epoch(model, dataloader, optimizer, device, pad_token_id):\n    model.train()\n    total_loss = 0\n    for input_ids, target_ids in tqdm(dataloader):\n        input_ids = input_ids.to(device)\n        target_ids = target_ids.to(device)\n\n        optimizer.zero_grad()\n        logits = model(input_ids)  # (B, seq_len, vocab_size)\n\n        logits_flat = logits.view(-1, logits.size(-1))       # (B*seq_len, vocab_size)\n        targets_flat = target_ids.view(-1)                   # (B*seq_len)\n\n        loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=pad_token_id)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef evaluate(model, dataloader, device, pad_token_id):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for input_ids, target_ids in dataloader:\n            input_ids = input_ids.to(device)\n            target_ids = target_ids.to(device)\n\n            logits = model(input_ids)\n            logits_flat = logits.view(-1, logits.size(-1))\n            targets_flat = target_ids.view(-1)\n\n            loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=pad_token_id)\n            total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef train(model, train_loader, val_loader, inv_vocab, epochs=10, lr=4e-4, save_path=\"gpt_model.pth\", pad_token_id=0, print_samples=3):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Wrap model with DataParallel if multiple GPUs available\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n        model = torch.nn.DataParallel(model)\n    \n    model.to(device)\n    optimizer = AdamW(model.parameters(), lr=lr)\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        train_loss = train_epoch(model, train_loader, optimizer, device, pad_token_id)\n        val_loss = evaluate(model, val_loader, device, pad_token_id)\n        print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n\n        # Print sample input, expected, and predicted outputs\n        model.eval()\n        with torch.no_grad():\n            for input_ids, target_ids in val_loader:\n                input_ids = input_ids.to(device)\n                logits = model(input_ids)  # (B, seq_len, vocab_size)\n                preds = logits.argmax(dim=-1)  # (B, seq_len)\n\n                inputs_text = decode_batch(input_ids, inv_vocab)\n                targets_text = decode_batch(target_ids, inv_vocab)\n                preds_text = decode_batch(preds, inv_vocab)\n                \n\n                print(\"\\nSamples:\")\n                for i in range(min(print_samples, len(inputs_text))):\n                    print(f\"Input    : {inputs_text[i]}\")\n                    print(f\"Expected : {targets_text[i]}\")\n                    print(f\"Predicted: {preds_text[i]}\")\n                    print(\"---\")\n                break  # only print from first batch\n\n        # Save model checkpoint (note DataParallel model.state_dict() contains \"module.\" prefix)\n        torch.save(model.state_dict(), \"Model.pth\")\n        print(f\"Model saved to {save_path}_epoch{epoch+1}.pth\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:43.120987Z","iopub.execute_input":"2025-08-09T17:26:43.121374Z","iopub.status.idle":"2025-08-09T17:26:43.134026Z","shell.execute_reply.started":"2025-08-09T17:26:43.121349Z","shell.execute_reply":"2025-08-09T17:26:43.133427Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(\"Building vocabulary...\")\nvocab = build_vocab(INPUT_CSV)\nsave_vocab(vocab, VOCAB_PATH, INV_VOCAB_PATH)\nprint(f\"Vocab saved. Size: {len(vocab)}\")\n\nprint(\"Encoding dataset...\")\nencode_dataset(INPUT_CSV, ENCODED_CSV, vocab)\nprint(f\"Encoded dataset saved to {ENCODED_CSV}\")\n\nprint(\"Splitting dataset into train and validation...\")\nsplit_dataset(ENCODED_CSV, TRAIN_CSV, VAL_CSV, val_ratio=VAL_RATIO)\n\nprint(\"Loading train dataset...\")\ntrain_dataset = GPTDataset(TRAIN_CSV, vocab, MAX_SEQ_LEN)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nprint(\"Loading validation dataset...\")\nval_dataset = GPTDataset(VAL_CSV, vocab, MAX_SEQ_LEN)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:43.134677Z","iopub.execute_input":"2025-08-09T17:26:43.134965Z","iopub.status.idle":"2025-08-09T17:26:46.435297Z","shell.execute_reply.started":"2025-08-09T17:26:43.134923Z","shell.execute_reply":"2025-08-09T17:26:46.434642Z"}},"outputs":[{"name":"stdout","text":"Building vocabulary...\nVocab saved. Size: 13025\nEncoding dataset...\nEncoded dataset saved to /kaggle/working/encoded_dataset.csv\nSplitting dataset into train and validation...\nTrain size: 90000, Validation size: 10000\nLoading train dataset...\nLoading validation dataset...\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"cfg = GPTConfig(\n    vocab_size=len(vocab),  # Make sure this matches your actual vocab size\n    d_model=512,            # Reduced from 768 for faster training\n    n_layers=6,             # Reduced from 12\n    n_heads=8,              # Reduced from 12\n    max_seq_len=MAX_SEQ_LEN, # Now using 64\n    dropout=0.1\n)\nmodel = GPTModel(cfg)\ndataset = GPTDataset(ENCODED_CSV, vocab, MAX_SEQ_LEN)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nvocab = build_vocab(INPUT_CSV)\ninv_vocab = {str(idx): word for word, idx in vocab.items()}\ntrain(\n    model,\n    train_loader,\n    val_loader,\n    inv_vocab,\n    epochs=100,              # Start with fewer epochs\n    lr=3e-4,               # Slightly lower learning rate\n    print_samples=3,       # Print more samples\n    pad_token_id=vocab[\"<PAD>\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:26:46.436178Z","iopub.execute_input":"2025-08-09T17:26:46.436404Z","iopub.status.idle":"2025-08-09T18:15:12.328306Z","shell.execute_reply.started":"2025-08-09T17:26:46.436377Z","shell.execute_reply":"2025-08-09T18:15:12.327500Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs for training\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:28<00:00, 15.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 5.3783, Val loss: 4.6436\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: and can be the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the the to and\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: and the of and\n---\nModel saved to gpt_model.pth_epoch1.pth\n\nEpoch 2/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 4.3100, Val loss: 3.9940\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: I can be the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the the the with\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with the statement: I\n---\nModel saved to gpt_model.pth_epoch2.pth\n\nEpoch 3/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 3.7587, Val loss: 3.5635\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: if can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: create the human-like the\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with the aspects I\n---\nModel saved to gpt_model.pth_epoch3.pth\n\nEpoch 4/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 3.3689, Val loss: 3.2406\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: if can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the the than which\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with the machines I\n---\nModel saved to gpt_model.pth_epoch4.pth\n\nEpoch 5/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 3.0441, Val loss: 2.9512\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the the investments you\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with the years, I\n---\nModel saved to gpt_model.pth_epoch5.pth\n\nEpoch 6/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 2.7583, Val loss: 2.7269\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the the of it\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with low techniques. I\n---\nModel saved to gpt_model.pth_epoch6.pth\n\nEpoch 7/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 2.5164, Val loss: 2.5205\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: a can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the the of and\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with the techniques, The\n---\nModel saved to gpt_model.pth_epoch7.pth\n\nEpoch 8/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 2.3117, Val loss: 2.3658\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: you can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the Salina of the\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with any techniques, Once\n---\nModel saved to gpt_model.pth_epoch8.pth\n\nEpoch 9/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 2.1301, Val loss: 2.2203\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all tables which\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch9.pth\n\nEpoch 10/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.9876, Val loss: 2.1162\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers which\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch10.pth\n\nEpoch 11/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.8672, Val loss: 2.0277\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all the which\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch11.pth\n\nEpoch 12/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.7717, Val loss: 1.9564\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the Supreem's the including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch12.pth\n\nEpoch 13/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.6847, Val loss: 1.8877\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all ads which\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new technologies I\n---\nModel saved to gpt_model.pth_epoch13.pth\n\nEpoch 14/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.6182, Val loss: 1.8380\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: be all the as\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch14.pth\n\nEpoch 15/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.5572, Val loss: 1.7939\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: make can easily the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the inactive the which\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch15.pth\n\nEpoch 16/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.5084, Val loss: 1.7574\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all the or\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch16.pth\n\nEpoch 17/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.4656, Val loss: 1.7233\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: be all ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch17.pth\n\nEpoch 18/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.4295, Val loss: 1.6984\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all the including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch18.pth\n\nEpoch 19/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.3951, Val loss: 1.6742\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all audio you\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch19.pth\n\nEpoch 20/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.3652, Val loss: 1.6531\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: if can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all audio including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch20.pth\n\nEpoch 21/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.3407, Val loss: 1.6389\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: make can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch21.pth\n\nEpoch 22/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.3152, Val loss: 1.6221\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers you\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch22.pth\n\nEpoch 23/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:28<00:00, 16.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.2934, Val loss: 1.5947\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: if can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch23.pth\n\nEpoch 24/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.2746, Val loss: 1.5852\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch24.pth\n\nEpoch 25/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.2550, Val loss: 1.5761\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the Supreem's people, you\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch25.pth\n\nEpoch 26/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.2372, Val loss: 1.5625\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: e can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the Supreem's answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch26.pth\n\nEpoch 27/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.2260, Val loss: 1.5554\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch27.pth\n\nEpoch 28/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.2119, Val loss: 1.5468\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch28.pth\n\nEpoch 29/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1996, Val loss: 1.5410\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the Supreem's people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch29.pth\n\nEpoch 30/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1884, Val loss: 1.5378\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch30.pth\n\nEpoch 31/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1766, Val loss: 1.5345\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch31.pth\n\nEpoch 32/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1694, Val loss: 1.5292\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch32.pth\n\nEpoch 33/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1592, Val loss: 1.5273\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, you\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch33.pth\n\nEpoch 34/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1505, Val loss: 1.5229\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch34.pth\n\nEpoch 35/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1429, Val loss: 1.5110\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: a can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch35.pth\n\nEpoch 36/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1365, Val loss: 1.5132\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: make can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch36.pth\n\nEpoch 37/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1306, Val loss: 1.5056\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch37.pth\n\nEpoch 38/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1241, Val loss: 1.5077\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: you can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch38.pth\n\nEpoch 39/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1172, Val loss: 1.5110\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch39.pth\n\nEpoch 40/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1102, Val loss: 1.5157\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: with can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch40.pth\n\nEpoch 41/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1063, Val loss: 1.5086\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch41.pth\n\nEpoch 42/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1007, Val loss: 1.5106\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch42.pth\n\nEpoch 43/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0931, Val loss: 1.5090\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch43.pth\n\nEpoch 44/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0907, Val loss: 1.5099\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch44.pth\n\nEpoch 45/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0853, Val loss: 1.5070\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can use the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch45.pth\n\nEpoch 46/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0819, Val loss: 1.5122\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch46.pth\n\nEpoch 47/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0799, Val loss: 1.4996\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch47.pth\n\nEpoch 48/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0736, Val loss: 1.5019\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch48.pth\n\nEpoch 49/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0697, Val loss: 1.5032\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch49.pth\n\nEpoch 50/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0661, Val loss: 1.5034\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: you can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch50.pth\n\nEpoch 51/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0644, Val loss: 1.4885\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch51.pth\n\nEpoch 52/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0603, Val loss: 1.4979\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch52.pth\n\nEpoch 53/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0564, Val loss: 1.4988\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch53.pth\n\nEpoch 54/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0543, Val loss: 1.4939\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: at can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch54.pth\n\nEpoch 55/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0511, Val loss: 1.4925\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the Supreem's people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch55.pth\n\nEpoch 56/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0486, Val loss: 1.5007\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch56.pth\n\nEpoch 57/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0458, Val loss: 1.4950\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch57.pth\n\nEpoch 58/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0424, Val loss: 1.4948\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the Supreem's ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch58.pth\n\nEpoch 59/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0398, Val loss: 1.4981\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch59.pth\n\nEpoch 60/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0371, Val loss: 1.4966\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch60.pth\n\nEpoch 61/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0348, Val loss: 1.5057\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: you can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch61.pth\n\nEpoch 62/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0337, Val loss: 1.4927\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all wells including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch62.pth\n\nEpoch 63/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0298, Val loss: 1.4948\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch63.pth\n\nEpoch 64/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0284, Val loss: 1.4972\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: you can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch64.pth\n\nEpoch 65/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0269, Val loss: 1.4831\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch65.pth\n\nEpoch 66/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0246, Val loss: 1.4907\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch66.pth\n\nEpoch 67/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0228, Val loss: 1.4967\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch67.pth\n\nEpoch 68/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0200, Val loss: 1.5063\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch68.pth\n\nEpoch 69/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0182, Val loss: 1.4994\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch69.pth\n\nEpoch 70/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0163, Val loss: 1.4930\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: in can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch70.pth\n\nEpoch 71/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0150, Val loss: 1.4958\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the ad's people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch71.pth\n\nEpoch 72/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0130, Val loss: 1.4973\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch72.pth\n\nEpoch 73/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:28<00:00, 15.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0147, Val loss: 1.4938\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch73.pth\n\nEpoch 74/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:28<00:00, 15.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0092, Val loss: 1.5077\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch74.pth\n\nEpoch 75/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0087, Val loss: 1.4964\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch75.pth\n\nEpoch 76/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0058, Val loss: 1.5026\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all ads including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch76.pth\n\nEpoch 77/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0061, Val loss: 1.5066\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch77.pth\n\nEpoch 78/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0017, Val loss: 1.5023\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: at can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch78.pth\n\nEpoch 79/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0020, Val loss: 1.4961\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch79.pth\n\nEpoch 80/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0013, Val loss: 1.4951\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch80.pth\n\nEpoch 81/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9977, Val loss: 1.4979\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch81.pth\n\nEpoch 82/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9964, Val loss: 1.4981\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: at can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch82.pth\n\nEpoch 83/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9964, Val loss: 1.4972\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch83.pth\n\nEpoch 84/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9939, Val loss: 1.4999\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch84.pth\n\nEpoch 85/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9929, Val loss: 1.5023\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch85.pth\n\nEpoch 86/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9930, Val loss: 1.5012\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch86.pth\n\nEpoch 87/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9903, Val loss: 1.5014\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch87.pth\n\nEpoch 88/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9890, Val loss: 1.4998\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch88.pth\n\nEpoch 89/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9895, Val loss: 1.5031\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch89.pth\n\nEpoch 90/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9866, Val loss: 1.5042\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch90.pth\n\nEpoch 91/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9846, Val loss: 1.5130\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch91.pth\n\nEpoch 92/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9866, Val loss: 1.4997\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch92.pth\n\nEpoch 93/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9839, Val loss: 1.4985\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch93.pth\n\nEpoch 94/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9825, Val loss: 1.5102\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch94.pth\n\nEpoch 95/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9811, Val loss: 1.5059\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch95.pth\n\nEpoch 96/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9807, Val loss: 1.4970\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: = can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the or people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques. I\n---\nModel saved to gpt_model.pth_epoch96.pth\n\nEpoch 97/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9796, Val loss: 1.5095\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: you can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all answers including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new sounds I\n---\nModel saved to gpt_model.pth_epoch97.pth\n\nEpoch 98/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9787, Val loss: 1.5066\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch98.pth\n\nEpoch 99/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9770, Val loss: 1.5083\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch99.pth\n\nEpoch 100/100\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 450/450 [00:27<00:00, 16.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9775, Val loss: 1.4947\n\nSamples:\nInput    : breast-fed, you can exponentiate\nExpected : you can exponentiate the\nPredicted: even can exponentiate the\n---\nInput    : to treat all people,\nExpected : treat all people, including\nPredicted: the all people, including\n---\nInput    : experimenting with new techniques.\nExpected : with new techniques. I\nPredicted: with new techniques, I\n---\nModel saved to gpt_model.pth_epoch100.pth\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport torch.nn.functional as F\n\n# Assume GPTConfig and GPTModel classes are already defined or imported here\n\ndef encode_input(text, vocab, max_seq_len):\n    tokens = [vocab[\"<BOS>\"]] + [vocab.get(w, vocab[\"<UNK>\"]) for w in text.split()] + [vocab[\"<EOS>\"]]\n    if len(tokens) > max_seq_len:\n        tokens = tokens[:max_seq_len]\n    else:\n        tokens += [vocab[\"<PAD>\"]] * (max_seq_len - len(tokens))\n    return torch.tensor([tokens], dtype=torch.long)\n\ndef decode_output(token_ids, inv_vocab):\n    words = []\n    for idx in token_ids:\n        word = inv_vocab.get(str(idx.item()), \"<UNK>\")\n        if word == \"<EOS>\":\n            break\n        if word not in (\"<PAD>\", \"<BOS>\"):\n            words.append(word)\n    return \" \".join(words)\n\ndef infer(model, input_text, vocab, inv_vocab, max_seq_len, max_new_tokens=20, temperature=1.0, top_k=10, device=None):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    model.to(device)\n\n    input_ids = encode_input(input_text, vocab, max_seq_len).to(device)\n    eos_token_id = vocab.get(\"<EOS>\")\n\n    with torch.no_grad():\n        generated_ids = model.generate(\n            input_ids,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            eos_token_id=eos_token_id,\n        )\n\n    output_ids = generated_ids[0].cpu()\n    return decode_output(output_ids, inv_vocab)\n\n# --- Usage Example ---\nVOCAB_PATH = \"/kaggle/working/vocab.json\"\nINV_VOCAB_PATH = \"/kaggle/working/inv_vocab.json\"\nMODEL_CHECKPOINT = \"/kaggle/working/Model.pth\"  # update path\n\n# Load vocabularies\nwith open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n    vocab = json.load(f)\nwith open(INV_VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n    inv_vocab = json.load(f)\n\n# Create config matching your trained model\ncfg = GPTConfig(\n    vocab_size=len(vocab),\n    d_model=512,\n    n_layers=6,\n    n_heads=8,\n    max_seq_len=5,\n    dropout=0.1,\n)\n\nmodel = GPTModel(cfg)\nstate_dict = torch.load(MODEL_CHECKPOINT, map_location=\"cpu\")\n\n# If trained with DataParallel, remove \"module.\" prefix\nfrom collections import OrderedDict\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k[7:] if k.startswith(\"module.\") else k\n    new_state_dict[name] = v\nmodel.load_state_dict(new_state_dict)\n\n# Example inference\ninput_text = \"Hello, how are you ?\"\noutput_text = infer(model, input_text, vocab, inv_vocab, cfg.max_seq_len)\nprint(\"Input:\", input_text)\nprint(\"Output:\", output_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T18:15:12.329266Z","iopub.execute_input":"2025-08-09T18:15:12.329654Z","iopub.status.idle":"2025-08-09T18:15:13.062924Z","shell.execute_reply.started":"2025-08-09T18:15:12.329634Z","shell.execute_reply":"2025-08-09T18:15:13.062241Z"}},"outputs":[{"name":"stdout","text":"Input: Hello, how are you ?\nOutput: would be happy to help, but\n","output_type":"stream"}],"execution_count":21}]}